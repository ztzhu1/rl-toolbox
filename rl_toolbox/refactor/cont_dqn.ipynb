{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import Compose, DoubleToFloat, ObservationNorm, TransformedEnv\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r'/home/ztzhu/codes/rl-toolbox')\n",
    "from rl_toolbox.utils.backend import get_device\n",
    "from rl_toolbox.utils.model_utils import save_model\n",
    "from rl_toolbox.utils.network_utils import mlp\n",
    "from rl_toolbox.visualization.monitor import plot_loss\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(\n",
    "    env_name=\"LunarLander-v2\",\n",
    "    seed=0,\n",
    "    init_stats_param: Union[int, OrderedDict] = 1000,\n",
    "    check_env=False,\n",
    "    **env_cfg\n",
    "):\n",
    "    env = GymEnv(env_name, **env_cfg)\n",
    "    env = TransformedEnv(\n",
    "        env,\n",
    "        Compose(\n",
    "            ObservationNorm(in_keys=[\"observation\"], standard_normal=True),\n",
    "            DoubleToFloat(in_keys=[\"observation\"]),\n",
    "        ),\n",
    "    )\n",
    "    env.set_seed(seed)\n",
    "    init_env(env, init_stats_param)\n",
    "\n",
    "    if check_env:\n",
    "        check_env_specs(env)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def init_env(env, init_stats_param=1000):\n",
    "    t = None\n",
    "    for _t in env.transform:\n",
    "        if isinstance(_t, ObservationNorm):\n",
    "            t = _t\n",
    "            break\n",
    "    if t is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(init_stats_param, int):\n",
    "        if not t.initialized:\n",
    "            t.init_stats(num_iter=init_stats_param)\n",
    "    else:\n",
    "        assert isinstance(init_stats_param, OrderedDict)\n",
    "        t.load_state_dict(init_stats_param)\n",
    "\n",
    "\n",
    "def make_actor(hidden_sizes, env):\n",
    "    sizes = (\n",
    "        [env.observation_spec[\"observation\"].shape[0]]\n",
    "        + hidden_sizes\n",
    "        + [env.action_spec.shape[0] * 2]  # loc and scale\n",
    "    )\n",
    "    actor = mlp(sizes, nn.Tanh, nn.Identity)\n",
    "    actor.append(NormalParamExtractor())\n",
    "    actor = TensorDictModule(actor, [\"observation\"], [\"loc\", \"scale\"])\n",
    "    actor = ProbabilisticActor(\n",
    "        actor,\n",
    "        [\"loc\", \"scale\"],\n",
    "        [\"action\"],\n",
    "        distribution_class=TanhNormal,\n",
    "        distribution_kwargs={\n",
    "            \"min\": env.action_spec.space.minimum,\n",
    "            \"max\": env.action_spec.space.maximum,\n",
    "        },\n",
    "        return_log_prob=True,\n",
    "    )\n",
    "    return actor\n",
    "\n",
    "\n",
    "def make_critic(hidden_sizes, env):\n",
    "    sizes = [env.observation_spec[\"observation\"].shape[0]] + hidden_sizes + [1]\n",
    "    critic = mlp(sizes, nn.Tanh, nn.Identity)\n",
    "    critic = ValueOperator(module=critic, in_keys=[\"observation\"])\n",
    "    return critic\n",
    "\n",
    "\n",
    "def make_collector(env, actor, epochs, steps_per_epoch, max_steps_per_traj):\n",
    "    collector = SyncDataCollector(\n",
    "        env,\n",
    "        actor,\n",
    "        frames_per_batch=steps_per_epoch,\n",
    "        total_frames=steps_per_epoch * epochs,\n",
    "        max_frames_per_traj=max_steps_per_traj,\n",
    "        reset_at_each_iter=True,\n",
    "        split_trajs=False,\n",
    "        device=device\n",
    "    )\n",
    "    return collector\n",
    "\n",
    "\n",
    "def make_buf(steps_per_epoch):\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyTensorStorage(steps_per_epoch, device=device), sampler=SamplerWithoutReplacement(),\n",
    "    )\n",
    "    return replay_buffer\n",
    "\n",
    "\n",
    "def make_loss(actor, critic, gamma=0.99, lam=0.95):\n",
    "    gae = GAE(gamma=gamma, lmbda=lam, value_network=critic, average_gae=True)\n",
    "    return gae, ClipPPOLoss(actor, critic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a580e78afd0647f7a03d9d42b1666683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ztzhu/codes/rl-toolbox/rl_toolbox/cont_dqn.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ztzhu/codes/rl-toolbox/rl_toolbox/cont_dqn.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m111\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ztzhu/codes/rl-toolbox/rl_toolbox/cont_dqn.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ztzhu/codes/rl-toolbox/rl_toolbox/cont_dqn.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(collector, \u001b[39m1\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ztzhu/codes/rl-toolbox/rl_toolbox/cont_dqn.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     l \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ztzhu/codes/rl-toolbox/rl_toolbox/cont_dqn.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m80\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchrl/collectors/collectors.py:731\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    730\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter \u001b[39m=\u001b[39m i\n\u001b[0;32m--> 731\u001b[0m tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout()\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_frames \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tensordict_out\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_frames \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m total_frames:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchrl/_utils.py:361\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _os_is_windows \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_distributed_rpc\u001b[39m.\u001b[39mPyRRef):\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_value()\n\u001b[0;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchrl/collectors/collectors.py:825\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrand_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensordict)\n\u001b[1;32m    824\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tensordict)\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensordict)\n\u001b[1;32m    828\u001b[0m \u001b[39m# we must clone all the values, since the step / traj_id updates are done in-place\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[1;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[0;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     skip_existing()\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[1;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[1;32m    251\u001b[0m ):\n\u001b[1;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/probabilistic.py:511\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.forward\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m@dispatch\u001b[39m(auto_batch_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    504\u001b[0m \u001b[39m@set_skip_existing\u001b[39m(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    510\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDictBase:\n\u001b[0;32m--> 511\u001b[0m     tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dist_params(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](tensordict_out, _requires_sample\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requires_sample)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/probabilistic.py:487\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist_params\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdefault_interaction_type\n\u001b[1;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m set_interaction_type(\u001b[39mtype\u001b[39m):\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m tds(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[1;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[0;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     skip_existing()\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[1;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[1;32m    251\u001b[0m ):\n\u001b[1;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/sequence.py:427\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs):\n\u001b[1;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule:\n\u001b[0;32m--> 427\u001b[0m         tensordict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_module(module, tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    428\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensordict_out\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or in_keys: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/sequence.py:407\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[0;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_module\u001b[39m(\n\u001b[1;32m    399\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m     module: TensorDictModule,\n\u001b[1;32m    401\u001b[0m     tensordict: TensorDictBase,\n\u001b[1;32m    402\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    403\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    405\u001b[0m         key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(include_nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39min_keys\n\u001b[1;32m    406\u001b[0m     ):\n\u001b[0;32m--> 407\u001b[0m         tensordict \u001b[39m=\u001b[39m module(tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    408\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[1;32m    409\u001b[0m         \u001b[39mfor\u001b[39;00m sub_td \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mtensordicts:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[1;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[0;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     skip_existing()\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[1;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[1;32m    251\u001b[0m ):\n\u001b[1;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/common.py:815\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    814\u001b[0m     tensors \u001b[39m=\u001b[39m (tensors,)\n\u001b[0;32m--> 815\u001b[0m tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_write_to_tensordict(tensordict, tensors, tensordict_out)\n\u001b[1;32m    816\u001b[0m \u001b[39mreturn\u001b[39;00m tensordict_out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/nn/common.py:760\u001b[0m, in \u001b[0;36mTensorDictModule._write_to_tensordict\u001b[0;34m(self, tensordict, tensors, tensordict_out, out_keys)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[39mfor\u001b[39;00m _out_key, _tensor \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(out_keys, tensors):\n\u001b[1;32m    759\u001b[0m     \u001b[39mif\u001b[39;00m _out_key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 760\u001b[0m         tensordict_out\u001b[39m.\u001b[39;49mset(_out_key, _tensor)\n\u001b[1;32m    761\u001b[0m \u001b[39mreturn\u001b[39;00m tensordict_out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/tensordict.py:3686\u001b[0m, in \u001b[0;36mTensorDict.set\u001b[0;34m(self, key, value, inplace)\u001b[0m\n\u001b[1;32m   3683\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m   3685\u001b[0m inplace \u001b[39m=\u001b[39m inplace \u001b[39mand\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m-> 3686\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_locked \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n\u001b[1;32m   3687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(TensorDictBase\u001b[39m.\u001b[39mLOCK_ERROR)\n\u001b[1;32m   3689\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_value(value)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensordict/tensordict.py:3216\u001b[0m, in \u001b[0;36mTensorDictBase.is_locked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3214\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   3215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_locked\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m-> 3216\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_is_locked\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m:\n\u001b[1;32m   3217\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_locked \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_locked\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = make_env(continuous=True, render_mode=None)\n",
    "actor = make_actor([128, 128], env)\n",
    "critic = make_critic([128, 128], env)\n",
    "collector = make_collector(env, actor, 300, 4000, 1000)\n",
    "buf = make_buf(4000)\n",
    "\n",
    "env.to(device)\n",
    "actor.to(device)\n",
    "critic.to(device)\n",
    "\n",
    "gae, loss_module = make_loss(actor, critic)\n",
    "opt = Adam(\n",
    "    [\n",
    "        {\"params\": actor.parameters(), \"lr\": 0.0004},\n",
    "        {\"params\": critic.parameters(), \"lr\": 0.001},\n",
    "    ]\n",
    ")\n",
    "scheduler = CosineAnnealingLR(opt, 300, 0.0)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "losses = []\n",
    "for epoch, data in tqdm(enumerate(collector, 1)):\n",
    "    l = []\n",
    "    for _ in range(80):\n",
    "        with torch.no_grad():\n",
    "            gae(data)\n",
    "        buf.extend(data)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loss_vals = loss_module(data)\n",
    "        loss = (\n",
    "            loss_vals[\"loss_objective\"]\n",
    "            + loss_vals[\"loss_critic\"]\n",
    "            + loss_vals[\"loss_entropy\"]\n",
    "        )\n",
    "        loss.backward()\n",
    "        l.append(loss.cpu().item())\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "    losses.append(np.mean(l))\n",
    "    plot_loss(ax, epoch, losses)\n",
    "    plt.show()\n",
    "\n",
    "    if epoch % 50 ==0:\n",
    "        save_model(r'/home/ztzhu/codes/rl-toolbox/rl_toolbox/models/testppo', epoch, {}, actor=actor.state_dict(), critic=critic.state_dict(), env=env.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in collector:\n",
    "    a=data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4000, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([4000]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n",
       "            batch_size=torch.Size([4000]),\n",
       "            device=cuda:0,\n",
       "            is_shared=True),\n",
       "        done: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "        loc: Tensor(shape=torch.Size([4000, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "                observation: Tensor(shape=torch.Size([4000, 8]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "                reward: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "                step_count: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "                truncated: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
       "            batch_size=torch.Size([4000]),\n",
       "            device=cuda:0,\n",
       "            is_shared=True),\n",
       "        observation: Tensor(shape=torch.Size([4000, 8]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        sample_log_prob: Tensor(shape=torch.Size([4000]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        scale: Tensor(shape=torch.Size([4000, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        step_count: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "        truncated: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
       "    batch_size=torch.Size([4000]),\n",
       "    device=cuda:0,\n",
       "    is_shared=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([4000, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        advantage: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([4000]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n",
       "            batch_size=torch.Size([4000]),\n",
       "            device=cuda:0,\n",
       "            is_shared=True),\n",
       "        done: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "        loc: Tensor(shape=torch.Size([4000, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "                observation: Tensor(shape=torch.Size([4000, 8]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "                reward: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "                step_count: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "                truncated: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
       "            batch_size=torch.Size([4000]),\n",
       "            device=cuda:0,\n",
       "            is_shared=True),\n",
       "        observation: Tensor(shape=torch.Size([4000, 8]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        sample_log_prob: Tensor(shape=torch.Size([4000]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        scale: Tensor(shape=torch.Size([4000, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        state_value: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
       "        step_count: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
       "        truncated: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
       "        value_target: Tensor(shape=torch.Size([4000, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
       "    batch_size=torch.Size([4000]),\n",
       "    device=cuda:0,\n",
       "    is_shared=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gae(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TensorDictKeysView(['action', 'step_count', 'observation', 'truncated', 'done', 'next', 'loc', 'scale', 'sample_log_prob', 'collector', 'state_value', 'advantage', 'value_target'],\n",
       "    include_nested=False,\n",
       "    leaves_only=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
